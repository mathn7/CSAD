var documenterSearchIndex = {"docs":
[{"location":"sujet-tp1/#TP-Projet-1-:-Se-familiariser-avec-l’Analyse-en-Composantes-Principales-(ACP)","page":"TP1","title":"TP-Projet 1 : Se familiariser avec l’Analyse en Composantes Principales (ACP)","text":"","category":"section"},{"location":"sujet-tp1/#Projet-de-calcul-scientifique-et-analyse-de-données-Modalités","page":"TP1","title":"Projet de calcul scientifique et analyse de données - Modalités","text":"","category":"section"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Ce document constitue la première partie du projet de calcul scientifique et analyse de données. Ce projet s’effectue par équipe de trois étudiants issus d’un même groupe de TD, et est découpé en trois parties. Pour chacune, il y aura un TP-projet durant lequel votre enseignant vous donnera des informations pratiques et/ou théoriques pour vous aider à la réalisation des travaux en autonomie. Tous les détails sur le déroulement de ce projet sont donnés sous Moodle, dans le document “Présentation projet 2021”","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"note: Nota Bene\nLes scripts Julia fournis dans cette partie s’appuient sur des jeux de données qui ont pour but d’illustrer les phénomènes décrits dans ce document, et ne sont bien sûr pas exhaustifs. Nous vous recommandons de créer vos propres jeux de données – aléatoirement avec les fonction rand et randn, en faisant varier les paramètres des lois, ou via des jeux de données trouvés sur internet –, et de noter dans le rapport vos remarques, interrogations et conclusions sur les observations que vous ferez en testant différents ensembles de données.","category":"page"},{"location":"sujet-tp1/#Partie-1-:-Visualiser-les-données","page":"TP1","title":"Partie 1 : Visualiser les données","text":"","category":"section"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Lorsque l’on cherche à analyser un jeu de données, un réflexe naturel est d’essayer de visualiser ces données. En effet, leur distribution dans l’espace et la façon dont elles sont agencées les unes par rapport aux autres peuvent être des indices précieux sur leurs interactions.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"“Voir” une donnée est un problème qui peut sembler trivial a priori, mais qui en fait ne l’est absolument pas dans la majeure partie des cas.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"En effet :","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Pour visualiser une donnée qui s’exprime grâce à un unique nombre (un entier, un réel, un bit ...), on peut placer cette donnée sur une droite munie d’un repère  (une origine et un vecteur de base).\nPour visualiser une donnée représentée par deux nombres – par exemple, la taille et le poids d’un être humain –, on peut la dessiner dans un plan, muni d’un repère (une origine et deux vecteurs de base).\nPour visualiser une donnée représentée par trois nombres – par exemple le niveau de rouge, de vert et de bleu d’un pixel dans une image au format RVB –, on se sert d’un espace à trois dimensions (ce qui, sur ordinateur, est déjà une vue de l’esprit).\nQuant à la visualisation des données exprimées par plus de trois nombres, cela ne peut être réalisé sans un traitement préalable.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"En bref, une donnée qui s'exprime comme un vecteur de mathbbR^p se visualise naturellement, à condition que p leq 3  Or, de très nombreuses données s'expriment avec plus de trois variables.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Considérons par exemple un étudiant représenté par ses notes dans chacune des épreuves du semestre. Avec cet exemple, on se ramène à un tableau à double entrée. En effet, si on veut représenter une population d'étudiants que l'on qualifie via leurs notes, on va naturellement créer un tableau dans lequel chaque ligne est un étudiant, chaque colonne est une épreuve, et dans chaque case (i j) on trouve la note que l'étudiant i a obtenue à l'épreuve j  On représente donc cette population par une matrice mathbfX in mathbbR^n times p avec n le nombre d'étudiants -ou individus-, et p le nombre d'épreuves -ou variables-.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"En analyse de données, un tel tableau mathbfX porte le nom de tableau des données. Cette représentation matricielle, alliée à des objets du domaine des statistiques, est extrêmement puissante car elle permet l'utilisation d'outils issus de l'algèbre linéaire, tels que l'Analyse en Composantes Principales (ACP).","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 1: Quelles étaient les données sur lesquelles on a appliqué l'ACP pendant le TP n° 1 d'analyse de données \"Espace de représentation des couleurs\"? Expliquer formellement à quoi correspondait le tableau de données mathbfX dans ce TP. Quelles étaient les dimensions des données?","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 2: Compléter le script visualisation.jl fourni, aux endroits précisés dans le code.","category":"page"},{"location":"sujet-tp1/#Partie-2-:-L’Analyse-en-Composantes-Principales","page":"TP1","title":"Partie 2 : L’Analyse en Composantes Principales","text":"","category":"section"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Pour répondre à la problématique de la visualisation des données, et d’une façon plus générale, pour réduire la dimension des individus sur lesquels on travaille, un outil classiquement utilisé est l’ACP. On peut l’expliquer globalement comme un changement de repère de l’espace des individus. Les axes de la nouvelle base sont triés dans l’ordre décroissant de l’information qu’ils permettent d’obtenir sur le jeu de données – ou, en d’autres termes, dans l’ordre décroissant de la proportion de contraste sur chacun de ces axes (voir document distribué lors du premier CTD d’Analyse de Données).","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Dans ce nouveau repère, l’origine correspond à l’individu moyen (i.e. la moyenne des individus). En projetant l’ensemble des individus sur le premier axe de la nouvelle base, on obtient la meilleure approximation du tableau des données dans un espace à une dimension. En projetant les individus sur le plan créé par les deux premiers axes –le second étant orthogonal au premier–, on obtient la meilleure approximation de l’ensemble des données dans un espace à deux dimensions, etc. Les nouveaux axes sont décorrélés deux à deux.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Rappels théoriques : Le tableau des données mathbfX in mathbbR^n times p est constitué de n individus représentés par p variables. Autrement dit, chaque ligne i de mathbfX correspond à une donnée (ou un individu) mathbfx_i in mathbbR^p  On note overlinemathbfx=frac1n sum_i=1^n mathbfx_i l'individu moyen. On appelle tableau centré des données la matrice  hspace*4cm mathbfX^c=leftbeginarrayc left(mathbfx_1-overlinemathbfxright)^top  vdots  left(mathbfx_n-overlinemathbfxright)^topendarrayright=leftbeginarrayc mathbfx_1^c top  vdots  mathbfx_n^c top endarrayright","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"(Image: )","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Figure 1 – Les points en rouge sont des vecteurs de mathbbR^2 . On les affiche dans le repère canonique dans lequel les données sont initialement exprimées. Puis on affiche dans ce repère le nouveau repère fourni par l’ACP. On voit que le premier axe correspond à la direction qui maximise la dispersion des données.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"La matrice de variance/covariance boldsymbolSigma=frac1nleft(mathbfX^cright)^top mathbfX^c in mathbbR^p times p indique le niveau de corrélation entre les axes de la base dans laquelle les individus sont initialement exprimées. Si ses éléments hors diagonale sont nuls, cela signifie que les variables sont décorrélées. Or, nous cherchons une base dans laquelle les variables sont décorrélées, et c'est la raison pour laquelle on cherche à diagonaliser boldsymbolSigma. Puisque boldsymbolSigma est symétrique, elle est diagonalisable dans une base orthonormée. Ainsi,  hspace*4cm exists mathbfU mathbfD in mathbbR^p times p mathbfSigma=mathbfU D U^top","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"avec U une matrice orthonormée, et D matrice diagonale, telle que leftd_1right geqleftd_2right geq ldots geqleftd_pright  U est la matrice de passage de la base des axes principaux vers la base canonique, dans laquelle les données sont initialement exprimées. En outre, vu que U est orthonormée, U ^top est la matrice de passage de la base canonique vers la base principale.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Si l'on veut exprimer un individu mathbfx_i dans ce nouveau repère, on applique d'abord le changement d'origine : mathbfx_i^c=mathbfx_i-overlinemathbfx suivi du changement de base, i.e. le produit par la matrice de passage de la base canonique vers la base principale : mathbfc_i=mathbfU^top mathbfx_i^c  Le tableau des données dans le nouveau repère sera la concaténation des transposées de tous les individus, i.e. la concaténation des  hspace*4cm mathbfc_i^top=left(mathbfU^top mathbfx_i^cright)^top=mathbfx_i^c top mathbfU forall i in1 ldots n","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"On a donc la matrice mathbfC=mathbfX^c mathbfU qui exprime bien les données dans le nouveau repère. La première colonne de mathbfC correspond à la projection des individus sur le premier axe principal : il s'agit de la première composante principale de mathbfX  De même, la deuxième colonne de mathbfC correspond à la projection des individus sur le deuxième axe principal : il s'agit de la deuxième composante principale de X. Les colonnes suivantes de C correspondent aux composantes principales suivantes de X.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 3: Compléter le script ACP.jl aux endroits précisés dans le code. Répondre dans le rapport à la question posée dans les commentaires du code.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 4: Comment peut-on quantifier l'information contenue dans les q premières composantes principales à partir de la matrice boldsymbolSigma ?","category":"page"},{"location":"sujet-tp1/#Partie-3-:-L’ACP-et-la-classification-de-données","page":"TP1","title":"Partie 3 : L’ACP et la classification de données","text":"","category":"section"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"La classification de données est un domaine dans lequel on cherche à partitionner un ensemble de données en classes ou clusters, c’est-à-dire, en sous-groupes de l’ensemble initial. Les données d’un même sous-groupe présentent une certaine homogénéité, qu’elles ne partagent pas avec les données des autres sous-groupes. Les clusters forment une partition de l’ensemble des données.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"En général, on définit une mesure de distance – ou à l’inverse, une mesure de proximité – sur l’espace des individus. Des données proches doivent appartenir au même cluster, des données éloignées doivent appartenir à des clusters différents. Quand les variables sont décorrélées, on peut utiliser la distance euclidienne pour mesurer l’écart entre les individus.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Si des groupes sont nettement définis, on les voit apparaı̂tre en visualisant les données avec l’ACP. Parfois, une seule composante suffit pour pouvoir définir chacune des classes. Parfois, plusieurs composantes sont nécessaires. Cela dépend notamment du nombre de classes. Deux exemples sont donnés à la Figure 2.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 5: Compléter le script classification.jl aux endroits précisés dans le code. Répondre dans le rapport aux questions posées dans les commentaires du code.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 6: Un fichier dataset.jld2 est fourni. Ce fichier contient un tableau de données mathbfX in mathbbR^n times p représentant n individus et p variables. Combien de classes d’individus peut-on identifier dans ce jeu de données ? Complétez le script Julia classes_individus.jl permettant de visualiser le plus clairement possible ces différents clusters.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Indices :","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Inspirez-vous du script classification.jl pour observer les classes.\nPour savoir combien de composantes principales sont nécessaires pour obtenir un taux d’information suffisant sur l’ensemble des données, on peut utiliser la proportion de contraste fournie par les éléments propres de la matrice de variance/covariance.\nIl est possible (mais pas obligatoire) d’utiliser la fonction kmeans de Julia (taper help kmeans pour plus d’information), qui correspond à l’algorithme des k-moyennes présenté en cours d’analyse de données (cf planches 37-51 du cours “Classification”). Cependant, la fonction kmeans doit être utilisée avec l’aide des observations faites sur les composantes principales, et la cohérence de ses résultats sur le jeu de données doit être vérifiée.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"De nombreuses raisons peuvent amener à chercher des classes parmi un groupe d’individus. En reprenant notre exemple des étudiants et des notes, on pourrait par exemple vouloir automatiser la détection d’étudiants en difficulté. Mais pourquoi ne pas se servir de ces données pour analyser les matières préférées des étudiants – a priori celles où ils ont les meilleures notes –, et les matières qui leur posent problème ? En bref, pourquoi se limiter à classifier les individus, alors qu’on pourrait aussi classifier les variables.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 7: Dans le fichier dataset.jld2, combien de classes de variables peut-on identifier ? Ecrire un nouveau script Julia classes_variables.jl permettant de visualiser clairement ces classes.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"(Image: )","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Figure 2 – (a) : composante principale d’un nuage de points contenant deux classes : cette première composante suffit à mettre en évidence les deux classes. Les images (b), (c), (d) représentent un même nuage de points de mathbbR^30 – contenant quatre classes – dans trois configurations différentes : (b) : première, deuxième et troisième composantes principales du nuage. (c) : le nuage dans le repère défini par les deux premiers axes principaux. (c) : le nuage dans le repère défini par les trois premiers axes principaux.","category":"page"},{"location":"sujet-tp1/#Partie-4-:-l’ACP-et-la-méthode-de-la-puissance-itérée","page":"TP1","title":"Partie 4 : l’ACP et la méthode de la puissance itérée","text":"","category":"section"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Dans les codes fournis, on récupère les éléments propres de Σ la matrice de variance/covariance par un simple appel à la fonction eigen de Julia, et on trie les vecteurs propres de Σ par ordre décroissant des valeurs propres associées, pour obtenir les axes principaux.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"On pourrait aussi utiliser la très classique méthode de la puissance itérée avec déflation, qui renverrait les couples propres directement dans l’ordre voulu. L’algorithme de la puissance itérée pour trouver le couple propre dominant – i.e sans l’opération de déflation – est présenté ci-dessous.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"METHODE DE LA PUISSANCE ITÉRÉE  hspace*05cmDonnées : une matrice mathbfM in mathbbR^p times p un vecteur normé mathbfx in mathbbR^p une tolérance epsilon0 itmax nombre max d'itérations. hspace*05cmSortie : (lambda mathbfx) in mathbbR times mathbbR^p couple propre dominant de mathbfM  hspace*05cmInitialisation : c v leftarrow textFALSE i leftarrow 0 lambda leftarrow mathbfx^top mathbfM mathbfx  hspace*05cm1. Tant que .NOT.cv: hspace*1cm mu leftarrow lambda  hspace*1cm mathbfx leftarrow mathbfM mathbfx  hspace*1cm mathbfx leftarrow mathbfx mathbfx  hspace*1cm lambda leftarrow mathbfx^top mathbfM mathbfx  hspace*1cm i leftarrow i+1  hspace*1cm c v leftarrowleft(fraclambda-mumu leq epsilonright)  textOR cdotleft(i geq i t_max right)  hspace*05cm2. Retourner (lambda mathrmx) ","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Les opérations de cet algorithme consistent majoritairement en des produits matrice-vecteur. Ainsi, la taille de la matrice mathbfM est déterminante pour arriver rapidement à la convergence, en terme de temps de calcul.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 8: Soit une matrice rectangulaire mathbfH in mathbbR^p times n  Expliquer pourquoi connaitre les éléments propres - i.e. les valeurs propres et les vecteurs propres - de mathbfH^top mathbfH permet de connaitre les éléments propres de mathbfH H^top","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Le script puissance_iteree.m contient deux instances de la méthode de la puissance itérée : une appliquée à une matrice mathbfA^top mathbfA l'autre appliquée à la matrice mathbfA mathbfA^top  La fonction cputime de Matlab permet de mesurer le temps d'exécution d'instructions.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 9: Compléter le script Matlab puissance_iteree.m aux endroits précisés dans le code. Répondre dans le rapport aux questions posées dans les commentaires du code.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 10: Lien avec l'ACP : est-il plus utile en théorie d'utiliser une fonction telle que eig ou la méthode de la puissance itérée pour calculer les éléments propres de boldsymbolSigma si le but est d'effectuer une ACP pour réduire les dimensions d'un espace? Justifier.","category":"page"},{"location":"sujet-tp1/","page":"TP1","title":"TP1","text":"Question 11: Si l'on choisit d'utiliser la méthode de la puissance itérée pour calculer les éléments propres de boldsymbolSigma sur quelle matrice doit-on appliquer la méthode pour minimiser le temps de calcul et la mémoire utilisée?","category":"page"},{"location":"sujet-tp2/#TP-Projet-2-:-Subspace-Iteration-Methods","page":"TP2","title":"TP-Projet 2 : Subspace Iteration Methods","text":"","category":"section"},{"location":"sujet-tp2/#/-Reminders-and-introduction","page":"TP2","title":"0/ Reminders and introduction","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Previously, we have seen that for reducing the dimension by the mean of Principal Component Analysis (PCA) we do not need the whole spectral decomposition of the symmetric variance/covariance matrix. Indeed we only need the leading eigenpairs which provide enough information about the data. We have implemented the power method, which was introduced in the Calcul Scientifique lectures, to compute the leading eigenpair. As it has been presented during the same lectures, it is possible to add a deflation process to this algorithm, in order to also compute the following eigenpairs. In this part of the project, we will see that this specific algorithm is not efficient in terms of performance. Then we will present a more efficient method called subspace iteration method, based on an object called Rayleigh quotient. We will study four variants of this method.","category":"page"},{"location":"sujet-tp2/#/-Limitations-of-the-power-method","page":"TP2","title":"1/ Limitations of the power method","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The basic power method, which was introduced in the Calcul Scientifique lectures, is recalled in Algorithm 1;it can be used to determine the eigenvector associated to the largest (in module) eigenvalue.  rule14cm05pt  Algorithm 1 Vector power method  rule14cm05pt  hspace*05cm Input: Matrix A in mathbbR^n times n  hspace*05cm Output: left(lambda_1 v_1right) eigenpair associated to the largest (in module) eigenvalue. hspace*05cm v in mathbbR^n given  hspace*05cm beta=v^T cdot A cdot v  hspace*05cm repeat  hspace*1cm y=A cdot v  hspace*1cm v=y y  hspace*1cm beta_text old =beta   hspace*1cm beta=v^T cdot A cdot vleftrighttext  ou  leftbeta=v^T cdot yright  hspace*05cm until beta-beta_oldbeta_oldvarepsilon  hspace*05cm lambda_1=beta and v_1=v  rule14cm05pt  By adding the deflation process, we are able to compute all the eigenpairs we need to reach a certain percentage of the trace of A. But this algorithm is not efficient at all in terms of computing time. A version of the power method with deflation is provided in the file power_v11.f90.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 1: Compare the running time of the subroutine deflatedpowermethod to compute a few eigenpairs with the running time of the Lapack subroutine dsyev(all you need to know about this subroutine is given in the file Manipulation_Tableaux_en_Fortran) . Comment.Question 2: What do you think to be the main drawback of the deflated power method in terms of computing time?","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Our objective is to extend the power method to compute a block of dominant eigenpairs.","category":"page"},{"location":"sujet-tp2/#/-Extending-the-power-method-to-compute-dominant-eigenspace-vectors","page":"TP2","title":"2/ Extending the power method to compute dominant eigenspace vectors","text":"","category":"section"},{"location":"sujet-tp2/#subspace-iter-v0:-a-basic-method-to-compute-a-dominant-eigenspace","page":"TP2","title":"subspace iter v0: a basic method to compute a dominant eigenspace","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The basic version of the method to compute an invariant subspace associated to the largest eigenvalues of a symmetric matrix A is described in Algorithm 2. This subspace is also called dominant eigenspace. Given a set of m orthonormal vectors V , the Algorithm 2 computes the eigenvectors associated with the m largest (in module) eigenvalues.  rule14cm05pt  Algorithm 2 Subspace iteration method v0 : the basic version  rule14cm05pt  hspace*05cm Input: Symmetric matrix A in mathbbR^n times n number of required eigenpairs m tolerance varepsilon and MaxIter (max nb of iterations)  hspace*05cmOutput: m dominant eigenvectors V_text out and the corresponding eigenvalues Lambda_text out. hspace*05cmGenerate a set of m orthonormal vectors V in mathbbR^n times m  k=0  hspace*05cmrepeat hspace*1cm k=k+1 hspace*1cm Y=A cdot V hspace*1cm H=V^T cdot A cdot Vleftright ou leftH=V^T cdot Yright hspace*1cm Compute a c c=A cdot V-V cdot H A hspace*1cm V longleftarrow orthonormalisation of the columns of Y  hspace*05cmuntil (k MaxIter or a c c leq varepsilon) hspace*05cmCompute the spectral decomposition X cdot Lambda_text out  cdot X^T=H where the eigenvalues of Hleft(operatornamediagleft(Lambda_text out right)right) are arranged in descending order of magnitude.  hspace*05cmCompute the corresponding eigenspace V_text out=V cdot X rule14cm05pt  This algorithm is very close to Algorithm 1:","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The process is mainly based on iterative products between the matrix A and the columns of V,\nInside the loop, the matrix H plays the same role than the scalar beta in Algorithm 1 .","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"There are however some differences:","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"An orthonormalisation of the columns of Y is realised at each iteration,\nThe relationship between both stopping criteria is not trivial,\nAt the end of the loop, the columns of V are not the eigenvectors of A  Additionnal operations have to be done after convergence to obtain the actual dominant eigenspace of A.","category":"page"},{"location":"sujet-tp2/#Orthonormalisation","page":"TP2","title":"Orthonormalisation","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The first question you may ask is: \"why do not simply apply Algorithm 1 on m initial vectors (matrix V ), instead of just one (vector v )?\"","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Actually, if one extends Algorithm 1 to iterate on such a matrix, then it will not tend to the expect result, i.e. V does not converge towards a matrix whose columns contain m different eigenvectors of A.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 3: Towards which matrix does V converge in such an algorithm ? Modify the Julia script puissance_iteree.jl from the TP-Projet 1 to check your conjecture.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"To avoid this phenomenon, an orthonormalisation of the columns of Y is realised at the end of each iteration in the Algorithm 2. The new set V is the result of this orthonormalisation.","category":"page"},{"location":"sujet-tp2/#Stopping-criterion","page":"TP2","title":"Stopping criterion","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Another difficulty to adapt Algorithm 1 in order to compute blocks of eigenpairs at once is the stopping criterion, because a set of eigenpairs must be tested for convergence and not only one vector.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The current stopping criterion in Algorithm 1 relies on the stagnation of the computed eigenvalue (it tests the fact that the computed eigenvalue no longer changes \"much\"). This choice has been done because it is computationally cheap. But this does not take into account the invariance of the eigenvector which is numerically more meaningful (see Optimisation Lectures).","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"One should notice that, in Algorithm 1, we have reached the convergence once v=x_1 and beta=lambda_1 and therefore A cdot v=beta cdot v  Then, a more meaningful stopping criterion for Algorithm 1 is to compute the relative invariance of the eigenvector that can be estimated as A cdot v-beta cdot v A  The Frobenius norm could then be used to compute the norm of a matrix.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"In Algorithm 2, we assume we have converged so that A V=V H  Thus, in our context, a possible measure of the convergence could be: A V-V H A  As before, the Frobenius norm can be used to compute the norm of both numerator and denominator.","category":"page"},{"location":"sujet-tp2/#Rayleigh-quotient","page":"TP2","title":"Rayleigh quotient","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Once the convergence is reached in Algorithm 2 V does not contain eigenvectors of A  But some spectral information about A can be extracted from H  Indeed, the loop in Algorithm 2 has converged once A cdot V=V cdot H  Then some eigenpairs of A can easily be obtained from eigenpairs of H if (lambda x) is an eigenpair for H then (lambda y=V cdot x) is an eigenpair for A^1. For a symmetric matrix A and a matrix V whose columns are orthonormal, such a matrix H=V^T cdot A cdot V is called a Rayleigh quotient. It will play a crucial role in the last two algorithms. For complements, see [1] .","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 4: We are looking at variants of the power method in order to avoid computing the whole spectral decomposition of the matrix A. But in Algorithm 2, a computation of the whole spectral decomposition of the matrix H is performed. Explain why it is not a problem by investigating the dimensions of H.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 5: In the file iter_v0.f90, fill in the subroutine subspaceiterv0 to obtain Algorithm 2.","category":"page"},{"location":"sujet-tp2/#subspace-iter-v1:-improved-version-making-use-of-Raleigh-Ritz-projection","page":"TP2","title":"subspace iter v1: improved version making use of Raleigh-Ritz projection","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Several modifications are needed to make the basic subspace iteration an efficient code.","category":"page"},{"location":"sujet-tp2/#First-improvements","page":"TP2","title":"First improvements","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"In our Principal Component Analysis application, it is more likely that the user asks to compute the smallest eigenspace such that the sum of the associated dominant eigenvalues is larger than a given percentage of the trace of the matrix A than a given number of eigenpairs. Let's call n_e v the number of the dominant eigenvalues needed.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Because this number n_e v is not known in advance, we chose to operate on a subspace whose dimension m is larger than n_e v. Note that if we reach the given size m of the subspace V without obtaining the percentage of the trace we have to stop: the method should be called again with a higher m.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Moreover to be able to stop the algorithm when the expected percentage is reached, an adaptation of the spectral decomposition of the Rayleigh quotient is used inside the subspace iteration.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"This adaptation is called the Rayleigh-Ritz projection procedure; an algorithmic description is given in Algorithm 3, for a symmetric positive definite matrix A as the variance/covariance matrix.  rule14cm05pt  Algorithm 3 Raleigh-Ritz projection  rule14cm05pt ","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"hspace*05cm Input: Matrix A in mathbbR^n times n and an orthonormal set of vectors V.  hspace*05cm Output: The approximate eigenvectors V_text out and the corresponding eigenvalues Lambda_text out.  hspace*05cm Compute the Rayleigh quotient H=V^T cdot A cdot V.  hspace*05cm Compute the spectral decomposition X cdot Lambda_o u t cdot X^T=H where the eigenvalues of Hleft(operatornamediagleft(Lambda_text outright)right) are arranged in descending order of magnitude. Compute V_o u t=V cdot X  rule14cm05pt ","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The algorithm of the subspace iter v1 is then:  rule14cm05pt  Algorithm 4 Subspace iteration method v1 with Raleigh-Ritz projection  rule14cm05pt  hspace*05cmInput: Symmetric matrix A in mathbbR^n times n tolerance varepsilon MaxIter (max nb of iterations) and PercentTrace the target percentage of the trace of A   hspace*05cmOutput: n_e v dominant eigenvectors V_text out and the corresponding eigenvalues Lambda_text out. hspace*05cmGenerate an initial set of m orthonormal vectors V in mathbbR^n times m  k=0  Percent Reached =0  hspace*05cmrepeat  hspace*1cm k=k+1 hspace*1cm Compute Y such that Y=A cdot V  hspace*1cm V longleftarrow orthonormalisation of the columns of Y hspace*1cm Rayleigh-Ritz projection applied on matrix A and orthonormal vectors V  hspace*1cm Convergence analysis step: save eigenpairs that have converged and update Percent Reached  hspace*05cmuntil ( Percent Reached PercentTrace or n_e v=m or k MaxIter )  rule14cm05pt ","category":"page"},{"location":"sujet-tp2/#Convergence-analysis-step","page":"TP2","title":"Convergence analysis step","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"A convergence analysis step is performed immediately after the Rayleigh-Ritz Projection step; its goal is to determine which eigenvectors have converged at the current iteration k. We consider that the eigenvector j stored in the j_t h column of V has converged when hspace*35cm leftr_jright=leftA cdot V_j-Lambda_j cdot V_jright A leq varepsilon Convergence theory says the eigenvectors corresponding to the largest eigenvalues will converge more swiftly than those corresponding to smaller eigenvalues. For this reason, we should test convergence of the eigenvectors in the order j=12 ldots","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"we consider that we do not converge at this iteration with the first one to fail the test,\nit is also not useful to test again the vectors that converged at the previous iteration.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 6: In the file iter_v1.f90 or subspaceiterv1.jl, identify all the steps of Algorithm 4.","category":"page"},{"location":"sujet-tp2/#/-Toward-an-efficient-solver:-subspace-iter-v2-and-subspace-iter-v3","page":"TP2","title":"3/ Toward an efficient solver: subspace iter v2 and subspace iter v3","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Two ways of improving the efficiency of the solver are proposed. Our aim is to build an algorithm that combines both the block approach and the deflation method in order to speed-up the convergence of the solver.","category":"page"},{"location":"sujet-tp2/#Block-approach-(subspace-iter-v2)","page":"TP2","title":"Block approach (subspace iter v2)","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Orthonormalisation is performed at each iteration and is quite costly. One simple way to accelerate the approach is to perform p products at each iteration (replace V=A cdot V (first step of the iteration) by leftV=A^p cdot Vright)  Note that this very simple acceleration method is applicable to all versions of the algorithm.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 7: what is the cost in term of flops of the computation of A^p then A^p cdot V  How organize differently this computation to reduce this cost?Question 8: Modify the subprogram subspaceiterv2 in file iter_v2.f90 to implement this acceleration (note that the initial code of this subprogram is the vi version of the method, the only difference is the input p ) (once may want to first develop the Julia version; it is possible, starting from the code Julia of v1).","category":"page"},{"location":"sujet-tp2/#Deflation-method-(subspace-iter-v3)","page":"TP2","title":"Deflation method (subspace iter v3)","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Because the columns of V converge in order, we can freeze the converged columns of V. This freezing results in significant savings in the matrix-vector (V=A cdot V) the orthonormalisation and Rayleigh-Ritz Projection steps.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"Specifically, suppose the first n b c^2 columns of V have converged, and partition V=leftV_c V_n cright where V_c has n b c columns and V_n c has m-n b c columns ^3  Then, we can form the matrix leftV_c A cdot V_n cright which is the same as if we multiply V by A  However, we still need to orthogonalise V_n c with respect to the frozen vectors V_c by first orthogonalising V_n c against V_c and then against itself. Finally, the Rayleigh-Ritz Projection step can also be limited to the columns V_n c of V.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 9: Running the program main with option disp=2, displays the accuracy of a vector when it converges and also at the end of the computation to check if everything is ok. Explain why, with subspaceiterv1 method, this accuracy differs for some of the vectors.Question 10: Try to anticipate what will occur with the subspaceiterv2 method Question 11: Copy the subprogram subspaceiterv2 into a subprogram subspaceiterv3 to implement this deflation (once may want to first develop the Julia version; it is possible, starting from the code Julia of v1).","category":"page"},{"location":"sujet-tp2/#/-TO-DO:-Numerical-experiments","page":"TP2","title":"4/ TO DO: Numerical experiments","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"The Fortran code can be compiled by typing the make command (a \"Makefile\" is provided). This will generate an executable file named \"main\", which executes the driver program. Everything you need to know to run your main program is in the README.txt file in the sources. To run your program, you have to set up several parameters (see also the README. txt file):","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"the solver (the LAPACK DSYEV, the deflated power method, subspaceiterv0, subspaceiterv1, subspaceiterv2 and subspaceiterv3 versions);\nthe size n of the matrix mathrmA whose eigenpairs have to be computed;\nthe type of matrix to be generated: the program can generate 4 types of matrices. Each type has different spectral properties and will therefore yield a different convergence behaviour for the subspace iteration variants;\nm the maximum number of eigenpairs (deflated power method) or the dimension of the invariant subspace (subspace iteration methods);\nthe percentage of the trace needed (only useful for the deflated power iteration and subspaceiter-v1 subspaceiterv2 and subspaceiter_v3 variants), to be expressed as a value between 0.0 and 1.0 );","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 12: Observe the behaviour of the subprograms you wrote in Section 3.1 when increasing p. Explain your results.","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 13: What are the differences between the 4 types of matrices ? Create some figures that show the eigenvalue distribution of these different types. The spectrum of a tested matrix is returned by the main program (cf the README.txt to know how). Feel free to create these figures with Julia, OpenOffice Calc,...","category":"page"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"note: TODO\nQuestion 14: Compare the performances of the algorithms you have implemented and those provided (including DSYEV) for different types and sizes of matrices.","category":"page"},{"location":"sujet-tp2/#/-Bibliography","page":"TP2","title":"5/ Bibliography","text":"","category":"section"},{"location":"sujet-tp2/","page":"TP2","title":"TP2","text":"G. W. Stewart. Matrix Algorithms: Volume 2, Eigensystems. Society for Industrial and Applied Mathe-matics (SIAM), 2001.","category":"page"},{"location":"sujet-tp3/#TP-Projet-3-:-Application-de-l’ACP-:-les-\"Eigenfaces\"","page":"TP3","title":"TP-Projet 3 : Application de l’ACP : les \"Eigenfaces\"","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Ce projet s’inspire d’un article intitulé Eigenfaces for recognition, écrit par Turk et Pentland et publié dans le Journal of Cognitive Neuroscience en 1991.","category":"page"},{"location":"sujet-tp3/#Description-des-données","page":"TP3","title":"Description des données","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Vous disposez de n images de visages d'un ensemble d'individus. Chaque individu est photographié sous le même nombre de postures faciales (gauche, face, trois quart face, etc.). Chacune de ces n images en niveaux de gris est stockée dans une matrice bidimensionnelle de taille 480 times 640  Ces n images constituent les images d'apprentissage. En les vectorisant, vous pouvez donc représenter ces images par des vecteurs colonnes de mathbbR^p où p=480 times 640=307200 est le nombre de pixels commun à toutes les images. Alors que dans le mathrmTP 1 chaque pixel d'une image couleur constitue un point de mathbbR^3 ici c'est chaque image qui constitue un point d'un espace affine mathbbR^p de dimension très élevée.","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"(Image: )","category":"page"},{"location":"sujet-tp3/#Exercice-1-:-analyse-en-composantes-principales","page":"TP3","title":"Exercice 1 : analyse en composantes principales","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"La matrice des données mathrmX de taille n times p contient sur chaque ligne la transposée d'une image vectorisée. Lancez le script donnees.jl afin de créer cette matrice et de la stocker dans un fichier au format Matlab, de nom donnees.jld2.","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Attention, pour le TP, seuls 4 individus sur 37 et 4 postures sur 6 sont sélectionnées pour faire partie de la base d'apprentissage ; il faudra bien entendu considérer un plus grande nombre d'individus et de postures pour les tests de performance","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"(Image: )","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"FIGURE 2 - Les \"eigenfaces\"","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Complétez le script exercice_1.jl, qui vise à calculer les axes principaux des images d’apprentissage à partir des vecteurs propres associés aux n - 1 valeurs propres non nulles de la matrice de variance/covariance Sigma des données (comme ici on vous demande de calculer toutes les valeurs propres de la matrice (sauf une), le choix d’utiliser la fonction eigenva de soi) . Ces axes principaux sont appelés eigenfaces par Turk et Pentland, par contraction des mots anglais eigenvectors et faces.","category":"page"},{"location":"sujet-tp3/#Exercice-2-:-projection-des-images-sur-les-eigenfaces","page":"TP3","title":"Exercice 2 : projection des images sur les eigenfaces","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Une fois connues les n-1 eigenfaces, on peut calculer les composantes principales. Complétez le script exercice_2.jl, de manière à afficher les images d'apprentissage reconstruites à l'aide des q premières eigenfaces et des q premières composantes principales, pour q in0 n-1","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Attention : n'oubliez pas d'ajouter l'individu moyen.","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Ce script doit également afficher l'évolution, en fonction de q de la racine carrée de l'erreur quadratique moyenne ( Root Mean Square Error, ou RMSE) entre les images originales et les images ainsi reconstruites.","category":"page"},{"location":"sujet-tp3/#Exercice-3-:-application-à-la-reconnaissance-de-visages","page":"TP3","title":"Exercice 3 : application à la reconnaissance de visages","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Le script clusters.jl calcule les composantes principales des n images d'apprentissage, puis affiche sous la forme d'un nuage de n points de mathbbR^2 leurs deux premières composantes principales. Chaque couleur correspond à un même individu de la base d'apprentissage. Ce nuage fait apparaitre des groupes de points (ou clusters) de couleur uniforme, ce qui montre que chaque cluster correspond aux différentes postures d'un même individu. Il semble donc possible d'utiliser les eigenfaces pour la reconnaissance de visages (comme l'indique le titre de l'article ayant inspiré ce TP : Eigenfaces for recognition), en calculant les deux premières composantes principales d'une image, dite image de test, n'appartenant pas forcément à la base d'apprentissage, et en cherchant de quelle image d'apprentissage cette image est la plus proche, donc à quel individu elle correspond. (Image: ) Le script exercice_3.jl tire aléatoirement (à l’aide de la fonction randi de Matlab) une image de test,parmi les 37 personnes et les six postures faciales disponibles dans la base de données.","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"(Image: ) Figure 3 – Résultat d’une requête sur une base de visages.","category":"page"},{"location":"sujet-tp3/#Travail-en-séance","page":"TP3","title":"Travail en séance","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Question 1 : exercice_1.jl complété\nQuestion 2 : exercice_2.jl complété\nQuestion 3 : exercice_3.jl complété","category":"page"},{"location":"sujet-tp3/#Questions-sur-la-reconnaissance-de-visages","page":"TP3","title":"Questions sur la reconnaissance de visages","text":"","category":"section"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Evaluation de la reconnaissance :\nQuestion 4: Configurez un classifieur (type de classifieur 1 ppv, 3 ppv ou autre, ... vu en cours) en complétant le script exercice_3.jl. Le classifieur k-ppv a été vu au TP4 d'Analyse de Données.\nQuestion 5: A partir des résultats obtenus de votre classifieur et des labels des images tests, construisez la matrice de confusion afin d'évaluer la qualité de votre classifieur. Comment optimiser votre classifieur? (on pourra notamment regarder le choix du nombre de composantes principales)","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"N'oubliez pas que vous avez à disposition 37 individus et 6 postures","category":"page"},{"location":"sujet-tp3/","page":"TP3","title":"TP3","text":"Discussion :\nQuestion 6 : Compte tenu de la quantité de données fournie (nombre et taille des images), quelle est votre préconisation (algorithmique et informatique) pour le calcul des couples propres utiles à cette application?\nQuestion 7: Faut-il utiliser l'implantation cholesky (via eigen) ou les algorithmes \"subspace iteration\"? (vous pouvez éprouver votre implantation Julia de ces algorithmes dans ce cadre) (Cette année, vu les conditions et l’obligation du travail à distance, nous avons enlevé l’interfaçage Matlab-Fortran qui permettait d’appeler le code Fortran des approches ”subspace iteration” à partir du Matlab.)\nQuestion 8 supplémentaire : En travaillant sur tout ou partie de la Base d'Apprentissage, discutez de la pertinence (spectrale) de l'introduction de la couleur dans la reconnaissance des visages. Lancez le script donneescouleur.jl afin de créer cette matrice et de la stocker dans un fichier au format JLD2, de nom donneesCouleur.jld2.","category":"page"},{"location":"#Tps-projet-Calcul-Scientifique-et-Analyse-de-Données","page":"Accueil","title":"Tps projet Calcul Scientifique et Analyse de Données","text":"","category":"section"},{"location":"","page":"Accueil","title":"Accueil","text":"Le package CSAD constitue une solution du projet Calcul Scientifique et Analyse de Données pour l’année 2020-2021 de l'École INP-ENSEEIHT.","category":"page"},{"location":"#Auteurs-:-A.B-C.D","page":"Accueil","title":"Auteurs :  A.B C.D","text":"","category":"section"}]
}
