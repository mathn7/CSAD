<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>TP3 · CSAD.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.jpg" alt="CSAD.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">CSAD.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Accueil</a></li><li><span class="tocitem">Sujets</span><ul><li><a class="tocitem" href="../sujet-tp1/">TP1</a></li><li><a class="tocitem" href="../sujet-tp2/">TP2</a></li><li class="is-active"><a class="tocitem" href>TP3</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Sujets</a></li><li class="is-active"><a href>TP3</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>TP3</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mathn7/CSAD/blob/master/docs/src/sujet-tp3.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="TP-Projet-3-:-Application-de-l’ACP-:-les-&quot;Eigenfaces&quot;"><a class="docs-heading-anchor" href="#TP-Projet-3-:-Application-de-l’ACP-:-les-&quot;Eigenfaces&quot;">TP-Projet 3 : Application de l’ACP : les &quot;Eigenfaces&quot;</a><a id="TP-Projet-3-:-Application-de-l’ACP-:-les-&quot;Eigenfaces&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#TP-Projet-3-:-Application-de-l’ACP-:-les-&quot;Eigenfaces&quot;" title="Permalink"></a></h1><p>Ce projet s’inspire d’un article intitulé <em>Eigenfaces for recognition</em>, écrit par Turk et Pentland et publié dans le <em>Journal of Cognitive Neuroscience</em> en 1991.</p><h3 id="Description-des-données"><a class="docs-heading-anchor" href="#Description-des-données">Description des données</a><a id="Description-des-données-1"></a><a class="docs-heading-anchor-permalink" href="#Description-des-données" title="Permalink"></a></h3><p>Vous disposez de <span>$n$</span> images de visages d&#39;un ensemble d&#39;individus. Chaque individu est photographié sous le même nombre de postures faciales (gauche, face, trois quart face, etc.). Chacune de ces <span>$n$</span> images en niveaux de gris est stockée dans une matrice bidimensionnelle de taille <span>$480 \times 640 .$</span> Ces <span>$n$</span> images constituent les images d&#39;apprentissage. En les vectorisant, vous pouvez donc représenter ces images par des vecteurs colonnes de <span>$\mathbb{R}^{p},$</span> où <span>$p=480 \times 640=307200$</span> est le nombre de pixels commun à toutes les images. Alors que dans le <span>$\mathrm{TP} 1,$</span> chaque pixel d&#39;une image couleur constitue un point de <span>$\mathbb{R}^{3},$</span> ici c&#39;est chaque image qui constitue un point d&#39;un espace affine <span>$\mathbb{R}^{p}$</span> de dimension très élevée.</p><p><img src="../assets/tp3-img1.png" alt/></p><h3 id="Exercice-1-:-analyse-en-composantes-principales"><a class="docs-heading-anchor" href="#Exercice-1-:-analyse-en-composantes-principales">Exercice 1 : analyse en composantes principales</a><a id="Exercice-1-:-analyse-en-composantes-principales-1"></a><a class="docs-heading-anchor-permalink" href="#Exercice-1-:-analyse-en-composantes-principales" title="Permalink"></a></h3><p>La matrice des données <span>$\mathrm{X},$</span> de taille <span>$n \times p,$</span> contient sur chaque ligne la transposée d&#39;une image vectorisée. Lancez le script donnees.jl afin de créer cette matrice et de la stocker dans un fichier au format Matlab, de nom donnees.jld2.</p><p><strong>Attention, pour le TP, seuls 4 individus sur 37 et 4 postures sur 6 sont sélectionnées pour faire partie de la base d&#39;apprentissage ; il faudra bien entendu considérer un plus grande nombre d&#39;individus et de postures pour les tests de performance</strong></p><p><img src="../assets/tp3-img2.png" alt/></p><p>FIGURE 2 - Les &quot;eigenfaces&quot;</p><p>Complétez le script <em>exercice_1.jl</em>, qui vise à calculer les axes principaux des images d’apprentissage à partir des vecteurs propres associés aux <span>$n - 1$</span> valeurs propres non nulles de la matrice de variance/covariance <span>$\Sigma$</span> des données (comme ici on vous demande de calculer toutes les valeurs propres de la matrice (sauf une), le choix d’utiliser la fonction eigenva de soi) . Ces axes principaux sont appelés eigenfaces par Turk et Pentland, par contraction des mots anglais <em>eigenvectors</em> et <em>faces</em>.</p><h3 id="Exercice-2-:-projection-des-images-sur-les-eigenfaces"><a class="docs-heading-anchor" href="#Exercice-2-:-projection-des-images-sur-les-eigenfaces">Exercice 2 : projection des images sur les eigenfaces</a><a id="Exercice-2-:-projection-des-images-sur-les-eigenfaces-1"></a><a class="docs-heading-anchor-permalink" href="#Exercice-2-:-projection-des-images-sur-les-eigenfaces" title="Permalink"></a></h3><p>Une fois connues les <span>$n-1$</span> eigenfaces, on peut calculer les composantes principales. Complétez le script <em>exercice_2.jl</em>, de manière à afficher les images d&#39;apprentissage reconstruites à l&#39;aide des <span>$q$</span> premières eigenfaces et des <span>$q$</span> premières composantes principales, pour <span>$q \in[0, n-1]$</span></p><p><strong>Attention</strong> : n&#39;oubliez pas d&#39;ajouter l&#39;individu moyen.</p><p>Ce script doit également afficher l&#39;évolution, en fonction de <span>$q,$</span> de la racine carrée de l&#39;erreur quadratique moyenne ( <em>Root Mean Square Error</em>, ou RMSE) entre les images originales et les images ainsi reconstruites.</p><h3 id="Exercice-3-:-application-à-la-reconnaissance-de-visages"><a class="docs-heading-anchor" href="#Exercice-3-:-application-à-la-reconnaissance-de-visages">Exercice 3 : application à la reconnaissance de visages</a><a id="Exercice-3-:-application-à-la-reconnaissance-de-visages-1"></a><a class="docs-heading-anchor-permalink" href="#Exercice-3-:-application-à-la-reconnaissance-de-visages" title="Permalink"></a></h3><p>Le script <em>clusters.jl</em> calcule les composantes principales des <span>$n$</span> images d&#39;apprentissage, puis affiche sous la forme d&#39;un nuage de <span>$n$</span> points de <span>$\mathbb{R}^{2}$</span> leurs deux premières composantes principales. Chaque couleur correspond à un même individu de la <em>base d&#39;apprentissage</em>. Ce nuage fait apparaitre des groupes de points (ou <em>clusters</em>) de couleur uniforme, ce qui montre que chaque <em>cluster</em> correspond aux différentes postures d&#39;un même individu. Il semble donc possible d&#39;utiliser les <em>eigenfaces</em> pour la reconnaissance de visages (comme l&#39;indique le titre de l&#39;article ayant inspiré ce TP : <em>Eigenfaces for recognition</em>), en calculant les deux premières composantes principales d&#39;une image, dite <em>image de test</em>, n&#39;appartenant pas forcément à la base d&#39;apprentissage, et en cherchant de quelle image d&#39;apprentissage cette image est la plus proche, donc à quel individu elle correspond. <img src="../assets/tp3-img3.png" alt/> Le script <em>exercice_3.jl</em> tire aléatoirement (à l’aide de la fonction randi de Matlab) une image de test,parmi les 37 personnes et les six postures faciales disponibles dans la base de données.</p><p><img src="../assets/tp3-img4.png" alt/> Figure 3 – Résultat d’une requête sur une base de visages.</p><h3 id="Travail-en-séance"><a class="docs-heading-anchor" href="#Travail-en-séance">Travail en séance</a><a id="Travail-en-séance-1"></a><a class="docs-heading-anchor-permalink" href="#Travail-en-séance" title="Permalink"></a></h3><ul><li><strong>Question 1</strong> : <em>exercice_1.jl</em> complété</li><li><strong>Question 2</strong> : <em>exercice_2.jl</em> complété</li><li><strong>Question 3</strong> : <em>exercice_3.jl</em> complété</li></ul><h3 id="Questions-sur-la-reconnaissance-de-visages"><a class="docs-heading-anchor" href="#Questions-sur-la-reconnaissance-de-visages">Questions sur la reconnaissance de visages</a><a id="Questions-sur-la-reconnaissance-de-visages-1"></a><a class="docs-heading-anchor-permalink" href="#Questions-sur-la-reconnaissance-de-visages" title="Permalink"></a></h3><ol><li><strong>Evaluation de la reconnaissance :</strong><ul><li><strong>Question 4:</strong> Configurez un classifieur (type de classifieur 1 ppv, 3 ppv ou autre, ... vu en cours) en complétant le script <em>exercice_3.jl</em>. Le classifieur k-ppv a été vu au TP4 d&#39;Analyse de Données.</li><li><strong>Question 5:</strong> A partir des résultats obtenus de votre classifieur et des labels des images tests, construisez la matrice de confusion afin d&#39;évaluer la qualité de votre classifieur. Comment optimiser votre classifieur? (on pourra notamment regarder le choix du nombre de composantes principales)</li></ul></li></ol><p><strong>N&#39;oubliez pas que vous avez à disposition 37 individus et 6 postures</strong></p><ol><li><strong>Discussion :</strong><ul><li><strong>Question 6</strong> : Compte tenu de la quantité de données fournie (nombre et taille des images), quelle est votre préconisation (algorithmique et informatique) pour le calcul des couples propres utiles à cette application?</li><li><strong>Question 7</strong>: Faut-il utiliser l&#39;implantation cholesky (via <em>eigen</em>) ou les algorithmes &quot;subspace iteration&quot;? (vous pouvez éprouver votre implantation Julia de ces algorithmes dans ce cadre) (Cette année, vu les conditions et l’obligation du travail à distance, nous avons enlevé l’interfaçage Matlab-Fortran qui permettait d’appeler le code Fortran des approches ”subspace iteration” à partir du Matlab.)</li></ul></li><li><strong>Question 8 supplémentaire</strong> : En travaillant sur tout ou partie de la Base d&#39;Apprentissage, discutez de la pertinence (spectrale) de l&#39;introduction de la couleur dans la reconnaissance des visages. Lancez le script <em>donneescouleur.jl</em> afin de créer cette matrice et de la stocker dans un fichier au format JLD2, de nom <em>donneesCouleur.jld2</em>.</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sujet-tp2/">« TP2</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 12 August 2020 10:18">Wednesday 12 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
